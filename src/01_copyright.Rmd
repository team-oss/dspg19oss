---
title: "Copyright Records"
author: "José Bayoán Santiago Calderón"
date: "2019-05-20"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Summary

Obtain all copyright records in the

U.S. Copyright Office. n.d. “Copyright Public Records Catalog Online. 1978 to Present.” US Library of Congress. Public Catalog. https://cocatalog.loc.gov/.

Time Coverage: Last 20 years

Type of record: Computer File

The data is public domain and open to harvest

- Curtesy policy
  - [off-peak time 21:00-06:00](https://public.resource.org/copyright.gov/index.html)
  - [10 requests per minute](https://www.loc.gov/legal/).
  - Let [Mr. Billy Hoppis](mailto:Billy%20Hoppis<cbhop@loc.gov>?subject=Data%20Harvest) know about the data harvest to monitor potential stress to the server

No bulk download functionality at present.

Search results are limited to a maximum of 10,000 records.

## Overview

The strategy will be to scrape records using the public portal in the following manner,

1. Use RSelenium to cache options that use JavaScript (date, document type)
2. Limit searches by date in order to avoid any query with more than 10,000 records
3. Use the required search query term as the letters of the alphabet to allow crawling all possible records
4. Use URL modifications to allow displaying all records in a single page
5. Download all records in the Full Record format to the server

## Data Harvest

Set-Up

```{r}
# Housekeeping ----
library(RSelenium)
library(stringr)
library(magrittr)
library(assertthat)
library(purrr)
library(data.table)
library(readr)
library(dplyr)
library(DBI)
# Initiate Web Driver ----
remDr <- remoteDriver(browserName = "chrome")
```

Storing results from a server query

- A query has URL encoded parameters
  - Some parameters may be modified
  - Other parameters are passed via an ID query through JavaScript (cannot be modified during the connection)
  - Sample query link: https://cocatalog.loc.gov/cgi-bin/Pwebrecon.cgi?PostSearchSortBy1=NULL&HID=15739438&HID=15744485&PostSearchSortBy2=NULL&Search_Arg=A&Search_Code=TALL&ti=1%2C0&CNT=10000&PID=S2XsVoljELJ6H0pO1s4cO16BnYQ_&SEQ=20190520172617&REC=1&RD=0&SAVE=Format+for+Print%2FSave&RC=1&MAILADDY=&EMAILADDRESS=None&LIMITBUTTON=0
  - URL passed to the server has a max lenght (about 500 records limit per request)

```{r}
#' Saves the raw results for copyright records of type "Computer File" in a given year for a given letter as search term ("/data/copyright/original/2000A1.html")
#'
#' @param remoteDriver remDr
#' @param character year
#' @param integer letter: 1 => "A", 26 => "Z"
#' @examples
#' scrape_copyright_records(year = "2000", letter = 1L)
scrape_copyright_records <- function(remDr, year, letter) {
  # JavaScript ----
  remDr$open()
  remDr$navigate("https://cocatalog.loc.gov/")
  set_search_limits <- remDr$findElement(using = "css",
                                         value = "body > form > table:nth-child(1) > tbody > tr:nth-child(2) > td > div > div > table > tbody > tr > td:nth-child(3) > div > a > img")
  set_search_limits$clickElement()
  item_type <- remDr$findElement(using = "xpath",
                                 value = "/html/body/form/table/tbody/tr/td/div/table/tbody/tr[7]/td[2]/div/select/option[10]")
  item_type$clickElement()
  date_input <- remDr$findElement(using = "xpath",
                                  value = "/html/body/form/table/tbody/tr/td/div/table/tbody/tr[5]/td[2]/div/input[1]")
  date_input$sendKeysToElement(list(year))
  set_search_limits_submit <- remDr$findElement(using = "xpath",
                                                value = "/html/body/form/table/tbody/tr/td/div/table/tbody/tr[10]/td/div/font/input[4]")
  set_search_limits_submit$clickElement()
  records_per_page <- remDr$findElement(using = "xpath",
                                        value = "/html/body/form/table[1]/tbody/tr[2]/td/div/div/table/tbody/tr/td[1]/select/option[1]")
  records_per_page$clickElement()
  search_for <- remDr$findElement(using = "xpath",
                                  value = "/html/body/form/table[1]/tbody/tr[1]/td/table[2]/tbody/tr[2]/td/table/tbody/tr[1]/td/font/b/input")
  search_for$sendKeysToElement(list(LETTERS[letter]))
  begin_search <- remDr$findElement(using = "xpath",
                                    value = "/html/body/form/table[1]/tbody/tr[2]/td/div/div/table/tbody/tr/td[2]/div/input[2]")
  begin_search$clickElement()
  # Verify that the maximum number of records for the query is below the limit ----
  records_on_page <- remDr$findElement(using = "xpath",
                                       value = "/html/body/table/tbody/tr[3]/td") %>%
    (function(.) .$getElementText()) %>%
    getElement(name = 1L) %>%
    str_extract(pattern = "\\d+(?= entries.)") %>%
    as.integer()
  assert_that(records_on_page <= 1e4)
  # All records on one page ----
  new_url <- str_replace(string = remDr$getCurrentUrl(),
                         pattern = "(?<=CNT=).*(?=&HIST)",
                         replacement = "10000")
  remDr$navigate(url = new_url)
  page_url <- remDr$getCurrentUrl() %>%
    getElement(name = 1L)
  # Server Query ----
  records_all_on_page <- remDr$findElement(using = "css",
                                           "body > form > center > center:nth-child(10) > table > tbody > tr:nth-child(3) > td:nth-child(1) > input[type=RADIO]:nth-child(1)")
  records_all_on_page$clickElement()
  export <- remDr$findElement(using = "css",
                              "body > form > center > center:nth-child(10) > table > tbody > tr:nth-child(2) > td:nth-child(2) > input[type=SUBMIT]")
  export$clickElement()
  server_url <- remDr$getCurrentUrl()[[1L]]
  # Get all records ID ----
  copyright_numbers <- str_extract_all(string = server_url,
                                       pattern = "(?<=HID=)\\d+") %>%
    getElement(name = 1L)
  # Build Queries ----
  queries <- map_chr(.x = seq(to = records_on_page, by = 500L),
                     .f = function(x) str_c("HID=", str_c(copyright_numbers[x:min(x + 499L, records_on_page)], collapse = "&HID="))) %>%
    str_c(str_extract(string = server_url, pattern = "^.*?(?=HID=)"),
          .,
          str_extract(string = server_url, pattern = "&PostSearchSortBy.*"))
  # Run queries and save on disk ----
  for (query in seq_along(queries)) {
    remDr$navigate(url = queries[query])
    output <- remDr$getPageSource() %>%
      getElement(name = 1L)
    path <- str_c(dirname(path = getwd()), "/data/copyright/original/", year, LETTERS[letter], query, ".html")
    file.create(path)
    write(x = output, file = path)
  }
  # Close Web Driver ----
  remDr$close()
  }
```

Run scrapper for a given year

```{r}
# year <- "YYYY"
start_time = Sys.time()
# for (i in seq_along(LETTERS)) {
#   scrape_copyright_records(remDr = remDr, year = year, letter = i)
#   Sys.sleep(time = 5)
# }
end_time <- Sys.time()
end_time - start_time
```

## Data Parsing

These functions parse the raw data into the tabular representation

```{r}
# For each record, it finds the field entries and values ----
parse_record <- function(record) {
  record <- str_split(string = record, pattern = "\n") %>%
    unlist()
  if (any(str_detect(record, pattern = "^\\w.*?(?=:)"))) {
    fieldnames <- str_extract_all(string = record, pattern = "^\\w.*?(?=:)", simplify = TRUE) %>%
      subset(!(. %in% "")) %>%
      as.vector()
    fieldvalues <- vector(mode = "character", length = length(fieldnames))
    for (i in seq_along(fieldnames)) {
      startline <- detect_index(.x = record, .f = function(string) str_detect(string = string, pattern = str_c("^", fieldnames[i], ":")))
      endline <- detect_index(.x = record[startline:length(record)], .f = function(string) str_detect(string = string, pattern = "")) + startline
      fieldvalues[i] <- str_c(record[startline:endline], collapse = " ") %>%
        str_extract(pattern = str_c("(?<=^", fieldnames[i], ":).*")) %>%
        str_trim() %>%
        str_replace_all(pattern = "\\s+", replacement = " ") %>%
        str_replace_all(pattern = "&amp;", replacement = "&")
      }
    output <- data.table(t(fieldvalues))
    colnames(output) <- fieldnames
    output
    }
  }
# For each file, it splits the various records and parse each one and then combine these ----
parse_file <- function(filename) {
  records <- read_file(filename) %>%
    str_split(pattern = "={80}") %>%
    getElement(name = 1L) %>%
    `[`(1L:min(500L, length(x = .))) %>%
    map_df(.f = parse_record)
  }
# It parses each file in the directory and combines all records in one tabular representation
parse_files <- function() {
  path <- str_c(dirname(path = getwd()), "/", "data", "/", "copyright", "/", "original", "/")
  start_time = Sys.time()
  output <- map_df(.x = str_c(path, list.files(path = path)), .f = parse_file)
  end_time <- Sys.time()
  print(end_time - start_time)
  output
  }
# output <- parse_files()
```

The tabular representation is then processed and uploaded to the database

- Remove duplicates
- Split compound fields
- Uploads to database

```{r}
# Clean dataset ----
# copyright <- output %>%
#   unique() %>%
#   mutate(`registration_number` = str_extract(string = `Registration Number / Date`, pattern = "\\w+"),
#          date = str_extract(string = `Registration Number / Date`, pattern = "(?<= / )\\w+")) %>%
#   select(-c(`Registration Number / Date`, `Type of Work`)) %>%
#   arrange(`Copyright Claimant`, date, Title)
# colnames(x = copyright) <- names(x = copyright) %>%
#   str_to_lower() %>%
#   str_replace_all(pattern = "\\s+", replacement = "_")
# Upload to database ----
copyright_to_bd <- function(copyright) {
  conn <- dbConnect(drv = RPostgreSQL::PostgreSQL(),
                    dbname = "oss",
                    host = "127.0.0.1",
                    port = "5433",
                    user = Sys.getenv("db_userid"),
                    password = Sys.getenv("db_pwd"))
  dbWriteTable(conn = conn,
              name = c(schema = "universe", name = "copyright"),
              value = copyright,
              row.names = FALSE,
              overwrite = TRUE)
  dbDisconnect(conn = conn)
}
# copyright_to_bd(copyright = copyright)
```
